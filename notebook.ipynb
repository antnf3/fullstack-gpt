{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain.prompts.few_shot import (\n",
    "    FewShotPromptTemplate,\n",
    "    FewShotChatMessagePromptTemplate,\n",
    ")\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
    "from langchain.prompts.example_selector.base import BaseExampleSelector\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1, streaming=True, callbacks=[StreamingStdOutCallbackHandler()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b = chat.predict(\"How many planets are there?\")\n",
    "# b\n",
    "\n",
    "template = PromptTemplate.from_template(\n",
    "    \"{country_a}와 {country_b}사이의 거리는 어떻게되나요?\"\n",
    ")\n",
    "prompt = template.format(country_a=\"맥시코\", country_b=\"인도\")\n",
    "chat.predict(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"당신은 지리 전문가입니다. 그리고 당신은 {language}로만 대답을 합니다.\",\n",
    "        ),\n",
    "        (\"ai\", \"안녕하세요 제 이름은 {name}입니다.\"),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"{country_a}와 {country_b}사이의 거리는 어떻게되나요? 그리고 당신의 이름은 무었입니까?\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "prompt = template.format_messages(\n",
    "    language=\"일본어\", name=\"길동\", country_a=\"일본\", country_b=\"한국\"\n",
    ")\n",
    "chat.predict_messages(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "\n",
    "class CommaOutputParser(BaseOutputParser):\n",
    "    def parse(self, text):\n",
    "        items = text.strip().split(\",\")\n",
    "        return list(map(str.strip, items))\n",
    "\n",
    "\n",
    "p = CommaOutputParser()\n",
    "p.parse(\" a ,  b ,  c ,   d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a list generating machine. Everything you are asked will be answered with a comma separated list of max {max_items} in lowercase. Do Not reply with anything else.\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "# prompt = template.format_messages(\n",
    "#     max_items=10,\n",
    "#     question=\"What are the planets\"\n",
    "# )\n",
    "prompt = template.format_messages(max_items=10, question=\"What are the colors\")\n",
    "result = chat.predict_messages(prompt)\n",
    "p = CommaOutputParser()\n",
    "p.parse(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = template | chat | CommaOutputParser()\n",
    "chain.invoke({\"max_items\": 5, \"question\": \"What are the pokemons?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chef_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a world-class international chef. You create easy to follow recipies for any type of cuisins with easy to find ingredients.\",\n",
    "        ),\n",
    "        (\"human\", \"I want to cook {cuisine} food\"),\n",
    "    ]\n",
    ")\n",
    "chef_chain = chef_prompt | chat\n",
    "# chef_chain.invoke({\n",
    "#     \"cuisine\":\"korean\"\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "veg_chef_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a vegetarian chef specialized on making traditional recipies vegetarian. You find a alternative ingredients and explain their preparation. You don't radically modify the recipe. If there is no alternative for a food just say you don't know how to replace it.\",\n",
    "        ),\n",
    "        (\"human\", \"{recipe}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "veg_chain = veg_chef_prompt | chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_chain = {\"recipe\": chef_chain} | veg_chain\n",
    "final_chain.invoke({\"cuisine\": \"korean\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 FewShotPromptTemplate\n",
    "\n",
    "# t = PromptTemplate.from_template(\"What is the capital of {country}\")\n",
    "# t.format(country=\"Korea\")\n",
    "\n",
    "# t=PromptTemplate(template=\"What is the capital of {country}\", input_variables=[\"country\"])\n",
    "# t.format(country=\"japan\")\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"What do you know about France?\",\n",
    "        \"answer\": \"\"\"\n",
    "Here is what I know:\n",
    "Capital: Paris\n",
    "Language: French\n",
    "Food: Wine and Cheese\n",
    "Currency: Euro\n",
    "\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Italy?\",\n",
    "        \"answer\": \"\"\"\n",
    "I know this:\n",
    "Capital: Rome\n",
    "Language: Italian\n",
    "Food: Pizza and Pasta\n",
    "Currency: Euro\n",
    "\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Greece?\",\n",
    "        \"answer\": \"\"\"\n",
    "I know this:\n",
    "Capital: Athens\n",
    "Language: Greek\n",
    "Food: Souvlaki and Feta Cheese\n",
    "Currency: Euro\n",
    "\"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "example_template = \"\"\"\n",
    "    Human: {question}\n",
    "    AI: {answer}\n",
    "\"\"\"\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(example_template)\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    "    suffix=\"Human: What do you know about {country}?\",\n",
    "    input_variables=[\"country\"],\n",
    ")\n",
    "\n",
    "chain = prompt | chat\n",
    "chain.invoke({\"country\": \"Germany\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 FewShotChatMessagePromptTemplate\n",
    "examples = [\n",
    "    {\n",
    "        \"country\": \"France\",\n",
    "        \"answer\": \"\"\"\n",
    "Here is what I know:\n",
    "Capital: Paris\n",
    "Language: French\n",
    "Food: Wine and Cheese\n",
    "Currency: Euro\n",
    "\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"country\": \"Italy\",\n",
    "        \"answer\": \"\"\"\n",
    "I know this:\n",
    "Capital: Rome\n",
    "Language: Italian\n",
    "Food: Pizza and Pasta\n",
    "Currency: Euro\n",
    "\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"country\": \"Greece\",\n",
    "        \"answer\": \"\"\"\n",
    "I know this:\n",
    "Capital: Athens\n",
    "Language: Greek\n",
    "Food: Souvlaki and Feta Cheese\n",
    "Currency: Euro\n",
    "\"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"What do you know about {country}?\"),\n",
    "        (\"ai\", \"{answer}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "example_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a geography expert, you give shorts answers.\"),\n",
    "        example_prompt,\n",
    "        (\"human\", \"What do you know about {country}?\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = final_prompt | chat\n",
    "chain.invoke({\"country\": \"Korea\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3 LengthBasedExampleSelector\n",
    "\n",
    "from typing import Any, Dict\n",
    "\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"What do you know about France?\",\n",
    "        \"answer\": \"\"\"\n",
    "Here is what I know:\n",
    "Capital: Paris\n",
    "Language: French\n",
    "Food: Wine and Cheese\n",
    "Currency: Euro\n",
    "\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Italy?\",\n",
    "        \"answer\": \"\"\"\n",
    "I know this:\n",
    "Capital: Rome\n",
    "Language: Italian\n",
    "Food: Pizza and Pasta\n",
    "Currency: Euro\n",
    "\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Greece?\",\n",
    "        \"answer\": \"\"\"\n",
    "I know this:\n",
    "Capital: Athens\n",
    "Language: Greek\n",
    "Food: Souvlaki and Feta Cheese\n",
    "Currency: Euro\n",
    "\"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "class RandomExampleSelector(BaseExampleSelector):\n",
    "    def __init__(self, examples):\n",
    "        self.examples = examples\n",
    "\n",
    "    def select_examples(self, input_variables):\n",
    "        from random import choice\n",
    "\n",
    "        return [choice(self.examples)]\n",
    "\n",
    "    def add_example(self, example):\n",
    "        self.examples.append(example)\n",
    "\n",
    "\n",
    "example_template = \"\"\"\n",
    "    Human: {question}\n",
    "    AI: {answer}\n",
    "\"\"\"\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(example_template)\n",
    "\n",
    "# example_selector = LengthBasedExampleSelector(\n",
    "#     examples=examples,\n",
    "#     example_prompt=example_prompt,\n",
    "#     max_length=10,\n",
    "# )\n",
    "example_selector = RandomExampleSelector(\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    example_selector=example_selector,\n",
    "    suffix=\"Human: What do you know about {country}?\",\n",
    "    input_variables=[\"country\"],\n",
    ")\n",
    "prompt.format(country=\"Korea\")\n",
    "\n",
    "# chain = prompt | chat\n",
    "# chain.invoke({\"country\": \"Germany\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.4 Serialization and Composition\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import load_prompt\n",
    "\n",
    "prompt = load_prompt(\"./prompt.yaml\")\n",
    "\n",
    "prompt.format(country=\"Germany\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts.pipeline import PipelinePromptTemplate\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()],\n",
    ")\n",
    "intro = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are a role playing assistant.\n",
    "    And you are impersonating a {character}\n",
    "\"\"\"\n",
    ")\n",
    "example = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    This is and example of how you talk:\n",
    "\n",
    "    Human: {example_question}\n",
    "    You: {example_answer}\n",
    "\"\"\"\n",
    ")\n",
    "start = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Start now!\n",
    "\n",
    "    Human: {question}\n",
    "    You: \n",
    "\"\"\"\n",
    ")\n",
    "final = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    {intro}\n",
    "\n",
    "    {example}\n",
    "\n",
    "    {start}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "prompts = [\n",
    "    (\"intro\", intro),\n",
    "    (\"example\", example),\n",
    "    (\"start\", start),\n",
    "]\n",
    "full_prompt = PipelinePromptTemplate(\n",
    "    final_prompt=final,\n",
    "    pipeline_prompts=prompts,\n",
    ")\n",
    "\n",
    "# full_prompt.format(\n",
    "#     character=\"Pirate\",\n",
    "#     example_question=\"What is your location?\",\n",
    "#     example_answer=\"Arrrrg! That is a secret!! Arg arg!!\",\n",
    "#     question=\"What is your fav food?\",\n",
    "# )\n",
    "\n",
    "chain = full_prompt | chat\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"character\": \"Pirate\",\n",
    "        \"example_question\": \"What is your location?\",\n",
    "        \"example_answer\": \"Arrrrg! That is a secret!! Arg arg!!\",\n",
    "        \"question\": \"What is your fav food?\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.5 Chaching\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.globals import set_llm_cache, set_debug\n",
    "from langchain.cache import InMemoryCache, SQLiteCache\n",
    "\n",
    "# set_llm_cache(InMemoryCache())\n",
    "set_llm_cache(SQLiteCache(\"cache.db\"))\n",
    "set_debug(True)\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    # streaming=True,\n",
    "    # callbacks=[\n",
    "    #     StreamingStdOutCallbackHandler()\n",
    "    # ]\n",
    ")\n",
    "chat.predict(\"How do you make italian pasta?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.6 Usage\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "with get_openai_callback() as usage:\n",
    "    a = chat.predict(\"What is the recipe for soju?\")\n",
    "    b = chat.predict(\"What is the recipe for bread?\")\n",
    "    print(a, b, \"\\n\")\n",
    "    print(usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.6 Serialization\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms.openai import OpenAI\n",
    "from langchain.llms.loading import load_llm\n",
    "\n",
    "# chat = OpenAI(\n",
    "# temperature=0.1,\n",
    "# max_tokens=450,\n",
    "# model=\"gpt-3.5-turbo-16k\"\n",
    "# )\n",
    "# chat.save(\"model.json\")\n",
    "\n",
    "chat = load_llm(\"model.json\")\n",
    "print(chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.0 ConversationBufferMemorry\n",
    "# 단점 - 대화 내용이 길어질수록 메모리도 계속 커지니깐 비효율적\n",
    "# 모델에게 매번 보내야될 대화기록이 길어짐 - 비용증가\n",
    "# 장점 - text completion할때 유용\n",
    "# 예측할때, 텍스트를 자동완성활때\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "memory.save_context({\"input\": \"Hi\"}, {\"output\": \"How are you?\"})\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 ConversationBufferWindowMemory\n",
    "\n",
    "# 장점 - 메모리 사이즈가 계속 늘어나지 않는다.\n",
    "# 단점 - 챗봇이 예전 대화를 기억하기 힘들다.\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(return_messages=True, k=4)\n",
    "\n",
    "\n",
    "def add_chat(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "\n",
    "add_chat(1, 1)\n",
    "add_chat(2, 2)\n",
    "add_chat(3, 3)\n",
    "add_chat(4, 4)\n",
    "add_chat(5, 5)\n",
    "add_chat(6, 6)\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 ConversationSummaryMemory\n",
    "\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=llm)\n",
    "\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "\n",
    "def get_history():\n",
    "    return memory.load_memory_variables({})\n",
    "\n",
    "\n",
    "add_message(\"Hi I'm Anderson, I live in South Korea\", \"Wow that is so cool!!\")\n",
    "add_message(\"South Korea is so pretty\", \"I wish I could go!!!\")\n",
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.3 ConversationSummaryBufferMemory\n",
    "\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.globals import set_debug\n",
    "\n",
    "set_debug(False)\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=10,\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "\n",
    "def get_history():\n",
    "    return memory.load_memory_variables({})\n",
    "\n",
    "\n",
    "add_message(\"Hi\", \"How are you?\")\n",
    "add_message(\"I'm Fine \", \"How are you?\")\n",
    "\n",
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationKGMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationKGMemory(\n",
    "    llm=llm,\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "\n",
    "add_message(\"Hi I'm Anderson, I live in South Korea\", \"Wow that is so cool!\")\n",
    "memory.load_memory_variables({\"inputs\": \"who is Anderson\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(\"Anderson likes kimchi\", \"Wow that is so cool!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.load_memory_variables({\"inputs\": \"What does anderson like\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.5 Memory on LLMChain\n",
    "\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    memory_key=\"chat_history\",\n",
    ")\n",
    "template = \"\"\"\n",
    "    You are a helpful AI talking to a human\n",
    "\n",
    "    {chat_history}\n",
    "    Human:{question}\n",
    "    You:\n",
    "\"\"\"\n",
    "chain = LLMChain(\n",
    "    llm=llm, memory=memory, prompt=PromptTemplate.from_template(template), verbose=True\n",
    ")\n",
    "\n",
    "chain.predict(question=\"My name is Anderson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.predict(question=\"I live in Seoul\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.predict(question=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.6 Chat Based Memory  ===> 오류발생 해결안됨\n",
    "# => 수정완료 => return_message 철자 오류 => return_messages\n",
    "\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI talking to a human\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm, memory=memory, prompt=prompt, verbose=True)\n",
    "\n",
    "chain.predict(question=\"My name is Anderson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.7 LCEL Based Memory => 이것도 오류 발생함......\n",
    "\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    # memory_key=\"chat_history\"  # default key 이므로 삭제 가능\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI talking to a human\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def load_memory(input):\n",
    "    print(input)\n",
    "    return memory.load_memory_variables({})[\"history\"]\n",
    "\n",
    "\n",
    "chain = RunnablePassthrough.assign(history=load_memory) | prompt | llm\n",
    "\n",
    "\n",
    "def invoke_chain(question):\n",
    "    result = chain.invoke({\"question\": question})\n",
    "    memory.save_context(\n",
    "        {\"input\": question},\n",
    "        {\"output\": result.content},\n",
    "    )\n",
    "\n",
    "    print(result)\n",
    "\n",
    "\n",
    "invoke_chain(\"My name is Anderson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invoke_chain(\"I live in Seoul\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invoke_chain(\"What is my name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 RAG (Retrieval-Augmented Generation) 검색-증강 생성\n",
    "# 6.1 Data Loaders and Splitters\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import (\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    CharacterTextSplitter,\n",
    ")\n",
    "\n",
    "\n",
    "# llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "# splitter = RecursiveCharacterTextSplitter(\n",
    "#     chunk_size=200,\n",
    "#     chunk_overlap=50\n",
    "# )\n",
    "splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "loader = UnstructuredFileLoader(\"./files/chapter_one.docx\")\n",
    "loader.load_and_split(text_splitter=splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 RAG (Retrieval-Augmented Generation) 검색-증강 생성\n",
    "# 6.4 Vector Store\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.storage import LocalFileStore\n",
    "\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "loader = UnstructuredFileLoader(\"./files/chapter_one.docx\")\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "cached_embeddir = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n",
    "vectorstore = Chroma.from_documents(docs, cached_embeddir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 RAG (Retrieval-Augmented Generation) 검색-증강 생성\n",
    "# 6.6 RetrievalQA\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores import Chroma, FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "loader = UnstructuredFileLoader(\"./files/chapter_one.docx\")\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "cached_embeddir = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n",
    "vectorstore = FAISS.from_documents(docs, cached_embeddir)\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"map_rerank\",  # \"map_reduce\", #\"refine\", #\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(),\n",
    ")\n",
    "\n",
    "# chain.run(\"Where does Winston live?\")\n",
    "chain.run(\"Describe Victory Mansions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 RAG (Retrieval-Augmented Generation) 검색-증강 생성\n",
    "# 6.8 Stuff LCEL Chain\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores import Chroma, FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "loader = UnstructuredFileLoader(\"./files/chapter_one.docx\")\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "cached_embeddir = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n",
    "vectorstore = FAISS.from_documents(docs, cached_embeddir)\n",
    "\n",
    "# chain = RetrievalQA.from_chain_type(\n",
    "#     llm=llm,\n",
    "#     chain_type=\"map_rerank\", #\"map_reduce\", #\"refine\", #\"stuff\",\n",
    "#     retriever=vectorstore.as_retriever(),\n",
    "# )\n",
    "retriver = vectorstore.as_retriever()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer questions using only the following context. If you don't know the answer just say you don't know, don't make it up:\\n\\n{context}\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "chain = {\"context\": retriver, \"question\": RunnablePassthrough()} | prompt | llm\n",
    "# chain.run(\"Where does Winston live?\")\n",
    "chain.invoke(\"Describe Victory Mansions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'documents': [Document(page_content='Part 1, Chapter 1\\nPart One\\n1\\nIt was a bright cold day in April, and the clocks were striking thirteen. Winston Smith, his chin nuzzled into his breast in an effort to escape the vile wind, slipped quickly through the glass doors of Victory Mansions, though not quickly enough to prevent a swirl of gritty dust from entering along with him.\\nThe hallway smelt of boiled cabbage and old rag mats. At one end of it a coloured poster, too large for indoor display, had been tacked to the wall. It depicted simply an enormous face, more than a metre wide: the face of a man of about forty-five, with a heavy black moustache and ruggedly handsome features. Winston made for the stairs. It was no use trying the lift. Even at the best of times it was seldom working, and at present the electric current was cut off during daylight hours. It was part of the economy drive in preparation for Hate Week. The flat was seven flights up, and Winston, who was thirty-nine and had a varicose ulcer above his right ankle, went slowly, resting several times on the way. On each landing, opposite the lift-shaft, the poster with the enormous face gazed from the wall. It was one of those pictures which are so contrived that the eyes follow you about when you move. BIG BROTHER IS WATCHING YOU, the caption beneath it ran.\\nInside the flat a fruity voice was reading out a list of figures which had something to do with the production of pig-iron. The voice came from an oblong metal plaque like a dulled mirror which formed part of the surface of the right-hand wall. Winston turned a switch and the voice sank somewhat, though the words were still distinguishable. The instrument (the telescreen, it was called) could be dimmed, but there was no way of shutting it off completely. He moved over to the window: a smallish, frail figure, the meagreness of his body merely emphasized by the blue overalls which were the uniform of the party. His hair was very fair, his face naturally sanguine, his skin roughened by coarse soap and blunt razor blades and the cold of the winter that had just ended.', metadata={'source': './files/chapter_one.docx'}), Document(page_content=\"The Ministry of Love was the really frightening one. There were no windows in it at all. Winston had never been inside the Ministry of Love, nor within half a kilometre of it. It was a place impossible to enter except on official business, and then only by penetrating through a maze of barbed-wire entanglements, steel doors, and hidden machine-gun nests. Even the streets leading up to its outer barriers were roamed by gorilla-faced guards in black uniforms, armed with jointed truncheons.\\nWinston turned round abruptly. He had set his features into the expression of quiet optimism which it was advisable to wear when facing the telescreen. He crossed the room into the tiny kitchen. By leaving the Ministry at this time of day he had sacrificed his lunch in the canteen, and he was aware that there was no food in the kitchen except a hunk of dark-coloured bread which had got to be saved for tomorrow's breakfast. He took down from the shelf a bottle of colourless liquid with a plain white label marked VICTORY GIN. It gave off a sickly, oily smell, as of Chinese ricespirit. Winston poured out nearly a teacupful, nerved himself for a shock, and gulped it down like a dose of medicine.\\nInstantly his face turned scarlet and the water ran out of his eyes. The stuff was like nitric acid, and moreover, in swallowing it one had the sensation of being hit on the back of the head with a rubber club. The next moment, however, the burning in his belly died down and the world began to look more cheerful. He took a cigarette from a crumpled packet marked VICTORY CIGARETTES and incautiously held it upright, whereupon the tobacco fell out on to the floor. With the next he was more successful. He went back to the living-room and sat down at a small table that stood to the left of the telescreen. From the table drawer he took out a penholder, a bottle of ink, and a thick, quarto-sized blank book with a red back and a marbled cover.\", metadata={'source': './files/chapter_one.docx'}), Document(page_content=\"For some reason the telescreen in the living-room was in an unusual position. Instead of being placed, as was normal, in the end wall, where it could command the whole room, it was in the longer wall, opposite the window. To one side of it there was a shallow alcove in which Winston was now sitting, and which, when the flats were built, had probably been intended to hold bookshelves. By sitting in the alcove, and keeping well back, Winston was able to remain outside the range of the telescreen, so far as sight went. He could be heard, of course, but so long as he stayed in his present position he could not be seen. It was partly the unusual geography of the room that had suggested to him the thing that he was now about to do.\\nBut it had also been suggested by the book that he had just taken out of the drawer. It was a peculiarly beautiful book. Its smooth creamy paper, a little yellowed by age, was of a kind that had not been manufactured for at least forty years past. He could guess, however, that the book was much older than that. He had seen it lying in the window of a frowsy little junk-shop in a slummy quarter of the town (just what quarter he did not now remember) and had been stricken immediately by an overwhelming desire to possess it. Party members were supposed not to go into ordinary shops ('dealing on the free market', it was called), but the rule was not strictly kept, because there were various things, such as shoelaces and razor blades, which it was impossible to get hold of in any other way. He had given a quick glance up and down the street and then had slipped inside and bought the book for two dollars fifty. At the time he was not conscious of wanting it for any particular purpose. He had carried it guiltily home in his briefcase. Even with nothing written in it, it was a compromising possession.\", metadata={'source': './files/chapter_one.docx'}), Document(page_content='Winston kept his back turned to the telescreen. It was safer, though, as he well knew, even a back can be revealing. A kilometre away the Ministry of Truth, his place of work, towered vast and white above the grimy landscape. This, he thought with a sort of vague distaste -- this was London, chief city of Airstrip One, itself the third most populous of the provinces of Oceania. He tried to squeeze out some childhood memory that should tell him whether London had always been quite like this. Were there always these vistas of rotting nineteenth-century houses, their sides shored up with baulks of timber, their windows patched with cardboard and their roofs with corrugated iron, their crazy garden walls sagging in all directions? And the bombed sites where the plaster dust swirled in the air and the willow-herb straggled over the heaps of rubble; and the places where the bombs had cleared a larger patch and there had sprung up sordid colonies of wooden dwellings like chicken-houses? But it was no use, he could not remember: nothing remained of his childhood except a series of bright-lit tableaux occurring against no background and mostly unintelligible.\\nThe Ministry of Truth -- Minitrue, in Newspeak -- was startlingly different from any other object in sight. It was an enormous pyramidal structure of glittering white concrete, soaring up, terrace after terrace, 300 metres into the air. From where Winston stood it was just possible to read, picked out on its white face in elegant lettering, the three slogans of the Party:\\nWAR IS PEACE\\nFREEDOM IS SLAVERY\\nIGNORANCE IS STRENGTH\\nThe Ministry of Truth contained, it was said, three thousand rooms above ground level, and corresponding ramifications below. Scattered about London there were just three other buildings of similar appearance and size. So completely did they dwarf the surrounding architecture that from the roof of Victory Mansions you could see all four of them simultaneously. They were the homes of the four Ministries between which the entire apparatus of government was divided. The Ministry of Truth, which concerned itself with news, entertainment, education, and the fine arts. The Ministry of Peace, which concerned itself with war. The Ministry of Love, which maintained law and order. And the Ministry of Plenty, which was responsible for economic affairs. Their names, in Newspeak: Minitrue, Minipax, Miniluv, and Miniplenty.', metadata={'source': './files/chapter_one.docx'})], 'question': 'Where does Winston go to work?'}\n"
     ]
    }
   ],
   "source": [
    "# 6 RAG (Retrieval-Augmented Generation) 검색-증강 생성\n",
    "# 6.9 Map Reduce LCEL Chain\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores import Chroma, FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "loader = UnstructuredFileLoader(\"./files/chapter_one.docx\")\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "cached_embeddir = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n",
    "vectorstore = FAISS.from_documents(docs, cached_embeddir)\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "map_doc_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            Use the following portion of a long document to see if any of the text is relevant to answer the question. Return any relevant text verbatim.\n",
    "            ------\n",
    "            {context}\n",
    "            \"\"\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "map_doc_chain = map_doc_prompt | llm\n",
    "\n",
    "\n",
    "def map_docs(inputs):\n",
    "    # print(inputs)\n",
    "    documents = inputs[\"documents\"]\n",
    "    question = inputs[\"question\"]\n",
    "\n",
    "    return \"\\n\\n\".join(\n",
    "        map_doc_chain.invoke(\n",
    "            {\"context\": doc.page_content, \"question\": question}\n",
    "        ).content\n",
    "        for doc in documents\n",
    "    )\n",
    "\n",
    "\n",
    "map_chain = {\n",
    "    \"documents\": retriever,\n",
    "    \"question\": RunnablePassthrough(),\n",
    "} | RunnableLambda(map_docs)\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "    Given the following extracted parts of a long document and a question, create a final answer.\",\n",
    "    \"If you don't know the answer, just say that you don't know. Don't try to make up an answer.\",\n",
    "    ------\n",
    "     {context}\n",
    "    \"\"\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "chain = {\"context\": map_chain, \"question\": RunnablePassthrough()} | final_prompt | llm\n",
    "# chain.invoke(\"Describe Victory Mansions\")\n",
    "chain.invoke(\"Where does Winston go to work?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
