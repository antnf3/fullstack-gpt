{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain.prompts.few_shot import (\n",
    "    FewShotPromptTemplate,\n",
    "    FewShotChatMessagePromptTemplate,\n",
    ")\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
    "from langchain.prompts.example_selector.base import BaseExampleSelector\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1, streaming=True, callbacks=[StreamingStdOutCallbackHandler()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b = chat.predict(\"How many planets are there?\")\n",
    "# b\n",
    "\n",
    "template = PromptTemplate.from_template(\n",
    "    \"{country_a}와 {country_b}사이의 거리는 어떻게되나요?\"\n",
    ")\n",
    "prompt = template.format(country_a=\"맥시코\", country_b=\"인도\")\n",
    "chat.predict(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"당신은 지리 전문가입니다. 그리고 당신은 {language}로만 대답을 합니다.\",\n",
    "        ),\n",
    "        (\"ai\", \"안녕하세요 제 이름은 {name}입니다.\"),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"{country_a}와 {country_b}사이의 거리는 어떻게되나요? 그리고 당신의 이름은 무었입니까?\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "prompt = template.format_messages(\n",
    "    language=\"일본어\", name=\"길동\", country_a=\"일본\", country_b=\"한국\"\n",
    ")\n",
    "chat.predict_messages(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "\n",
    "class CommaOutputParser(BaseOutputParser):\n",
    "    def parse(self, text):\n",
    "        items = text.strip().split(\",\")\n",
    "        return list(map(str.strip, items))\n",
    "\n",
    "\n",
    "p = CommaOutputParser()\n",
    "p.parse(\" a ,  b ,  c ,   d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a list generating machine. Everything you are asked will be answered with a comma separated list of max {max_items} in lowercase. Do Not reply with anything else.\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "# prompt = template.format_messages(\n",
    "#     max_items=10,\n",
    "#     question=\"What are the planets\"\n",
    "# )\n",
    "prompt = template.format_messages(max_items=10, question=\"What are the colors\")\n",
    "result = chat.predict_messages(prompt)\n",
    "p = CommaOutputParser()\n",
    "p.parse(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = template | chat | CommaOutputParser()\n",
    "chain.invoke({\"max_items\": 5, \"question\": \"What are the pokemons?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chef_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a world-class international chef. You create easy to follow recipies for any type of cuisins with easy to find ingredients.\",\n",
    "        ),\n",
    "        (\"human\", \"I want to cook {cuisine} food\"),\n",
    "    ]\n",
    ")\n",
    "chef_chain = chef_prompt | chat\n",
    "# chef_chain.invoke({\n",
    "#     \"cuisine\":\"korean\"\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "veg_chef_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a vegetarian chef specialized on making traditional recipies vegetarian. You find a alternative ingredients and explain their preparation. You don't radically modify the recipe. If there is no alternative for a food just say you don't know how to replace it.\",\n",
    "        ),\n",
    "        (\"human\", \"{recipe}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "veg_chain = veg_chef_prompt | chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_chain = {\"recipe\": chef_chain} | veg_chain\n",
    "final_chain.invoke({\"cuisine\": \"korean\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 FewShotPromptTemplate\n",
    "\n",
    "# t = PromptTemplate.from_template(\"What is the capital of {country}\")\n",
    "# t.format(country=\"Korea\")\n",
    "\n",
    "# t=PromptTemplate(template=\"What is the capital of {country}\", input_variables=[\"country\"])\n",
    "# t.format(country=\"japan\")\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"What do you know about France?\",\n",
    "        \"answer\": \"\"\"\n",
    "Here is what I know:\n",
    "Capital: Paris\n",
    "Language: French\n",
    "Food: Wine and Cheese\n",
    "Currency: Euro\n",
    "\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Italy?\",\n",
    "        \"answer\": \"\"\"\n",
    "I know this:\n",
    "Capital: Rome\n",
    "Language: Italian\n",
    "Food: Pizza and Pasta\n",
    "Currency: Euro\n",
    "\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Greece?\",\n",
    "        \"answer\": \"\"\"\n",
    "I know this:\n",
    "Capital: Athens\n",
    "Language: Greek\n",
    "Food: Souvlaki and Feta Cheese\n",
    "Currency: Euro\n",
    "\"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "example_template = \"\"\"\n",
    "    Human: {question}\n",
    "    AI: {answer}\n",
    "\"\"\"\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(example_template)\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    "    suffix=\"Human: What do you know about {country}?\",\n",
    "    input_variables=[\"country\"],\n",
    ")\n",
    "\n",
    "chain = prompt | chat\n",
    "chain.invoke({\"country\": \"Germany\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 FewShotChatMessagePromptTemplate\n",
    "examples = [\n",
    "    {\n",
    "        \"country\": \"France\",\n",
    "        \"answer\": \"\"\"\n",
    "Here is what I know:\n",
    "Capital: Paris\n",
    "Language: French\n",
    "Food: Wine and Cheese\n",
    "Currency: Euro\n",
    "\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"country\": \"Italy\",\n",
    "        \"answer\": \"\"\"\n",
    "I know this:\n",
    "Capital: Rome\n",
    "Language: Italian\n",
    "Food: Pizza and Pasta\n",
    "Currency: Euro\n",
    "\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"country\": \"Greece\",\n",
    "        \"answer\": \"\"\"\n",
    "I know this:\n",
    "Capital: Athens\n",
    "Language: Greek\n",
    "Food: Souvlaki and Feta Cheese\n",
    "Currency: Euro\n",
    "\"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"What do you know about {country}?\"),\n",
    "        (\"ai\", \"{answer}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "example_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a geography expert, you give shorts answers.\"),\n",
    "        example_prompt,\n",
    "        (\"human\", \"What do you know about {country}?\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = final_prompt | chat\n",
    "chain.invoke({\"country\": \"Korea\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3 LengthBasedExampleSelector\n",
    "\n",
    "from typing import Any, Dict\n",
    "\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"What do you know about France?\",\n",
    "        \"answer\": \"\"\"\n",
    "Here is what I know:\n",
    "Capital: Paris\n",
    "Language: French\n",
    "Food: Wine and Cheese\n",
    "Currency: Euro\n",
    "\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Italy?\",\n",
    "        \"answer\": \"\"\"\n",
    "I know this:\n",
    "Capital: Rome\n",
    "Language: Italian\n",
    "Food: Pizza and Pasta\n",
    "Currency: Euro\n",
    "\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Greece?\",\n",
    "        \"answer\": \"\"\"\n",
    "I know this:\n",
    "Capital: Athens\n",
    "Language: Greek\n",
    "Food: Souvlaki and Feta Cheese\n",
    "Currency: Euro\n",
    "\"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "class RandomExampleSelector(BaseExampleSelector):\n",
    "    def __init__(self, examples):\n",
    "        self.examples = examples\n",
    "\n",
    "    def select_examples(self, input_variables):\n",
    "        from random import choice\n",
    "\n",
    "        return [choice(self.examples)]\n",
    "\n",
    "    def add_example(self, example):\n",
    "        self.examples.append(example)\n",
    "\n",
    "\n",
    "example_template = \"\"\"\n",
    "    Human: {question}\n",
    "    AI: {answer}\n",
    "\"\"\"\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(example_template)\n",
    "\n",
    "# example_selector = LengthBasedExampleSelector(\n",
    "#     examples=examples,\n",
    "#     example_prompt=example_prompt,\n",
    "#     max_length=10,\n",
    "# )\n",
    "example_selector = RandomExampleSelector(\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    example_selector=example_selector,\n",
    "    suffix=\"Human: What do you know about {country}?\",\n",
    "    input_variables=[\"country\"],\n",
    ")\n",
    "prompt.format(country=\"Korea\")\n",
    "\n",
    "# chain = prompt | chat\n",
    "# chain.invoke({\"country\": \"Germany\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.4 Serialization and Composition\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import load_prompt\n",
    "\n",
    "prompt = load_prompt(\"./prompt.yaml\")\n",
    "\n",
    "prompt.format(country=\"Germany\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts.pipeline import PipelinePromptTemplate\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()],\n",
    ")\n",
    "intro = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are a role playing assistant.\n",
    "    And you are impersonating a {character}\n",
    "\"\"\"\n",
    ")\n",
    "example = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    This is and example of how you talk:\n",
    "\n",
    "    Human: {example_question}\n",
    "    You: {example_answer}\n",
    "\"\"\"\n",
    ")\n",
    "start = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Start now!\n",
    "\n",
    "    Human: {question}\n",
    "    You: \n",
    "\"\"\"\n",
    ")\n",
    "final = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    {intro}\n",
    "\n",
    "    {example}\n",
    "\n",
    "    {start}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "prompts = [\n",
    "    (\"intro\", intro),\n",
    "    (\"example\", example),\n",
    "    (\"start\", start),\n",
    "]\n",
    "full_prompt = PipelinePromptTemplate(\n",
    "    final_prompt=final,\n",
    "    pipeline_prompts=prompts,\n",
    ")\n",
    "\n",
    "# full_prompt.format(\n",
    "#     character=\"Pirate\",\n",
    "#     example_question=\"What is your location?\",\n",
    "#     example_answer=\"Arrrrg! That is a secret!! Arg arg!!\",\n",
    "#     question=\"What is your fav food?\",\n",
    "# )\n",
    "\n",
    "chain = full_prompt | chat\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"character\": \"Pirate\",\n",
    "        \"example_question\": \"What is your location?\",\n",
    "        \"example_answer\": \"Arrrrg! That is a secret!! Arg arg!!\",\n",
    "        \"question\": \"What is your fav food?\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.5 Chaching\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.globals import set_llm_cache, set_debug\n",
    "from langchain.cache import InMemoryCache, SQLiteCache\n",
    "\n",
    "# set_llm_cache(InMemoryCache())\n",
    "set_llm_cache(SQLiteCache(\"cache.db\"))\n",
    "set_debug(True)\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    # streaming=True,\n",
    "    # callbacks=[\n",
    "    #     StreamingStdOutCallbackHandler()\n",
    "    # ]\n",
    ")\n",
    "chat.predict(\"How do you make italian pasta?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.6 Usage\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "with get_openai_callback() as usage:\n",
    "    a = chat.predict(\"What is the recipe for soju?\")\n",
    "    b = chat.predict(\"What is the recipe for bread?\")\n",
    "    print(a, b, \"\\n\")\n",
    "    print(usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.6 Serialization\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms.openai import OpenAI\n",
    "from langchain.llms.loading import load_llm\n",
    "\n",
    "# chat = OpenAI(\n",
    "# temperature=0.1,\n",
    "# max_tokens=450,\n",
    "# model=\"gpt-3.5-turbo-16k\"\n",
    "# )\n",
    "# chat.save(\"model.json\")\n",
    "\n",
    "chat = load_llm(\"model.json\")\n",
    "print(chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.0 ConversationBufferMemorry\n",
    "# 단점 - 대화 내용이 길어질수록 메모리도 계속 커지니깐 비효율적\n",
    "# 모델에게 매번 보내야될 대화기록이 길어짐 - 비용증가\n",
    "# 장점 - text completion할때 유용\n",
    "# 예측할때, 텍스트를 자동완성활때\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "memory.save_context({\"input\": \"Hi\"}, {\"output\": \"How are you?\"})\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 ConversationBufferWindowMemory\n",
    "\n",
    "# 장점 - 메모리 사이즈가 계속 늘어나지 않는다.\n",
    "# 단점 - 챗봇이 예전 대화를 기억하기 힘들다.\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(return_messages=True, k=4)\n",
    "\n",
    "\n",
    "def add_chat(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "\n",
    "add_chat(1, 1)\n",
    "add_chat(2, 2)\n",
    "add_chat(3, 3)\n",
    "add_chat(4, 4)\n",
    "add_chat(5, 5)\n",
    "add_chat(6, 6)\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 ConversationSummaryMemory\n",
    "\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=llm)\n",
    "\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "\n",
    "def get_history():\n",
    "    return memory.load_memory_variables({})\n",
    "\n",
    "\n",
    "add_message(\"Hi I'm Anderson, I live in South Korea\", \"Wow that is so cool!!\")\n",
    "add_message(\"South Korea is so pretty\", \"I wish I could go!!!\")\n",
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.3 ConversationSummaryBufferMemory\n",
    "\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.globals import set_debug\n",
    "\n",
    "set_debug(False)\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=10,\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "\n",
    "def get_history():\n",
    "    return memory.load_memory_variables({})\n",
    "\n",
    "\n",
    "add_message(\"Hi\", \"How are you?\")\n",
    "add_message(\"I'm Fine \", \"How are you?\")\n",
    "\n",
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationKGMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationKGMemory(\n",
    "    llm=llm,\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "\n",
    "add_message(\"Hi I'm Anderson, I live in South Korea\", \"Wow that is so cool!\")\n",
    "memory.load_memory_variables({\"inputs\": \"who is Anderson\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(\"Anderson likes kimchi\", \"Wow that is so cool!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.load_memory_variables({\"inputs\": \"What does anderson like\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.5 Memory on LLMChain\n",
    "\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    memory_key=\"chat_history\",\n",
    ")\n",
    "template = \"\"\"\n",
    "    You are a helpful AI talking to a human\n",
    "\n",
    "    {chat_history}\n",
    "    Human:{question}\n",
    "    You:\n",
    "\"\"\"\n",
    "chain = LLMChain(\n",
    "    llm=llm, memory=memory, prompt=PromptTemplate.from_template(template), verbose=True\n",
    ")\n",
    "\n",
    "chain.predict(question=\"My name is Anderson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.predict(question=\"I live in Seoul\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.predict(question=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.6 Chat Based Memory  ===> 오류발생 해결안됨\n",
    "# => 수정완료 => return_message 철자 오류 => return_messages\n",
    "\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI talking to a human\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm, memory=memory, prompt=prompt, verbose=True)\n",
    "\n",
    "chain.predict(question=\"My name is Anderson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.7 LCEL Based Memory => 이것도 오류 발생함......\n",
    "\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    # memory_key=\"chat_history\"  # default key 이므로 삭제 가능\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI talking to a human\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def load_memory(input):\n",
    "    print(input)\n",
    "    return memory.load_memory_variables({})[\"history\"]\n",
    "\n",
    "\n",
    "chain = RunnablePassthrough.assign(history=load_memory) | prompt | llm\n",
    "\n",
    "\n",
    "def invoke_chain(question):\n",
    "    result = chain.invoke({\"question\": question})\n",
    "    memory.save_context(\n",
    "        {\"input\": question},\n",
    "        {\"output\": result.content},\n",
    "    )\n",
    "\n",
    "    print(result)\n",
    "\n",
    "\n",
    "invoke_chain(\"My name is Anderson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invoke_chain(\"I live in Seoul\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invoke_chain(\"What is my name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 RAG (Retrieval-Augmented Generation) 검색-증강 생성\n",
    "# 6.1 Data Loaders and Splitters\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import (\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    CharacterTextSplitter,\n",
    ")\n",
    "\n",
    "\n",
    "# llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "# splitter = RecursiveCharacterTextSplitter(\n",
    "#     chunk_size=200,\n",
    "#     chunk_overlap=50\n",
    "# )\n",
    "splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "loader = UnstructuredFileLoader(\"./files/chapter_one.docx\")\n",
    "loader.load_and_split(text_splitter=splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 RAG (Retrieval-Augmented Generation) 검색-증강 생성\n",
    "# 6.4 Vector Store\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.storage import LocalFileStore\n",
    "\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "loader = UnstructuredFileLoader(\"./files/chapter_one.docx\")\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "cached_embeddir = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n",
    "vectorstore = Chroma.from_documents(docs, cached_embeddir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 RAG (Retrieval-Augmented Generation) 검색-증강 생성\n",
    "# 6.6 RetrievalQA\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores import Chroma, FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "loader = UnstructuredFileLoader(\"./files/chapter_one.docx\")\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "cached_embeddir = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n",
    "vectorstore = FAISS.from_documents(docs, cached_embeddir)\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"map_rerank\",  # \"map_reduce\", #\"refine\", #\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(),\n",
    ")\n",
    "\n",
    "# chain.run(\"Where does Winston live?\")\n",
    "chain.run(\"Describe Victory Mansions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 RAG (Retrieval-Augmented Generation) 검색-증강 생성\n",
    "# 6.8 Stuff LCEL Chain\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores import Chroma, FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "loader = UnstructuredFileLoader(\"./files/chapter_one.docx\")\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "cached_embeddir = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n",
    "vectorstore = FAISS.from_documents(docs, cached_embeddir)\n",
    "\n",
    "# chain = RetrievalQA.from_chain_type(\n",
    "#     llm=llm,\n",
    "#     chain_type=\"map_rerank\", #\"map_reduce\", #\"refine\", #\"stuff\",\n",
    "#     retriever=vectorstore.as_retriever(),\n",
    "# )\n",
    "retriver = vectorstore.as_retriever()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer questions using only the following context. If you don't know the answer just say you don't know, don't make it up:\\n\\n{context}\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "chain = {\"context\": retriver, \"question\": RunnablePassthrough()} | prompt | llm\n",
    "# chain.run(\"Where does Winston live?\")\n",
    "chain.invoke(\"Describe Victory Mansions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 RAG (Retrieval-Augmented Generation) 검색-증강 생성\n",
    "# 6.9 Map Reduce LCEL Chain\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores import Chroma, FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "loader = UnstructuredFileLoader(\"./files/chapter_one.docx\")\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "cached_embeddir = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n",
    "vectorstore = FAISS.from_documents(docs, cached_embeddir)\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "map_doc_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            Use the following portion of a long document to see if any of the text is relevant to answer the question. Return any relevant text verbatim.\n",
    "            ------\n",
    "            {context}\n",
    "            \"\"\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "map_doc_chain = map_doc_prompt | llm\n",
    "\n",
    "\n",
    "def map_docs(inputs):\n",
    "    # print(inputs)\n",
    "    documents = inputs[\"documents\"]\n",
    "    question = inputs[\"question\"]\n",
    "\n",
    "    return \"\\n\\n\".join(\n",
    "        map_doc_chain.invoke(\n",
    "            {\"context\": doc.page_content, \"question\": question}\n",
    "        ).content\n",
    "        for doc in documents\n",
    "    )\n",
    "\n",
    "\n",
    "map_chain = {\n",
    "    \"documents\": retriever,\n",
    "    \"question\": RunnablePassthrough(),\n",
    "} | RunnableLambda(map_docs)\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "    Given the following extracted parts of a long document and a question, create a final answer.\",\n",
    "    \"If you don't know the answer, just say that you don't know. Don't try to make up an answer.\",\n",
    "    ------\n",
    "     {context}\n",
    "    \"\"\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "chain = {\"context\": map_chain, \"question\": RunnablePassthrough()} | final_prompt | llm\n",
    "# chain.invoke(\"Describe Victory Mansions\")\n",
    "chain.invoke(\"Where does Winston go to work?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"A {word} is a\")\n",
    "\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"openai-community/gpt2\",\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs={\"max_new_tokens\": 50},\n",
    ")\n",
    "\n",
    "chain = prompt | llm\n",
    "chain.invoke({\"word\": \"potato\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "def get_weather(lon, lat):\n",
    "    print(\"call function...\")\n",
    "\n",
    "\n",
    "function = {\n",
    "    \"name\": \"get_weather\",\n",
    "    \"description\": \"function that takes longitude and latitude to find the weather of a place\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"lon\": {\"type\": \"string\", \"description\": \"The longitude coordinate\"},\n",
    "            \"lan\": {\"type\": \"string\", \"description\": \"The latitude coordinate\"},\n",
    "        },\n",
    "    },\n",
    "    \"required\": [\"lon\", \"lat\"],\n",
    "}\n",
    "llm = ChatOpenAI(temperature=0.1).bind(function_call=\"auto\", functions=[function])\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"Who is the weather in {city}?\")\n",
    "\n",
    "chain = prompt | llm\n",
    "response = chain.invoke({\"city\": \"korea\"})\n",
    "args = response.additional_kwargs[\"function_call\"][\"arguments\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "r = json.loads(args)\n",
    "get_weather(r[\"lon\"], r[\"lan\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "function = {\n",
    "    \"name\": \"create_quiz\",\n",
    "    \"description\": \"function that takes a list of questions and answers and returns a quiz\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"questions\": {\n",
    "                \"type\": \"array\",\n",
    "                \"items\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"question\": {\n",
    "                            \"type\": \"string\",\n",
    "                        },\n",
    "                        \"answers\": {\n",
    "                            \"type\": \"array\",\n",
    "                            \"items\": {\n",
    "                                \"type\": \"object\",\n",
    "                                \"properties\": {\n",
    "                                    \"answer\": {\n",
    "                                        \"type\": \"string\",\n",
    "                                    },\n",
    "                                    \"correct\": {\n",
    "                                        \"type\": \"boolean\",\n",
    "                                    },\n",
    "                                },\n",
    "                                \"required\": [\"answer\", \"correct\"],\n",
    "                            },\n",
    "                        },\n",
    "                    },\n",
    "                    \"required\": [\"question\", \"answers\"],\n",
    "                },\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"questions\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1).bind(\n",
    "    function_call={\"name\": \"create_quiz\"},\n",
    "    functions=[function],\n",
    ")\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"Make a quiz about {city}\")\n",
    "\n",
    "chain = prompt | llm\n",
    "response = chain.invoke({\"city\": \"korea\"})\n",
    "args = response.additional_kwargs[\"function_call\"][\"arguments\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from pydub import AudioSegment\n",
    "import math\n",
    "\n",
    "\n",
    "def extract_audio_from_video(video_path, audio_path):\n",
    "    command = [\"ffmpeg\", \"-i\", video_path, \"-vn\", audio_path]\n",
    "    subprocess.run(command)\n",
    "\n",
    "\n",
    "def cut_audio_in_chunks(audio_path, minutes, dest_path):\n",
    "    track = AudioSegment.from_mp3(audio_path)\n",
    "    chunk_len = minutes * 60 * 1000\n",
    "    chunks = math.ceil(len(track) / chunk_len)\n",
    "    for i in range(chunks):\n",
    "        start_time = i * chunk_len\n",
    "        end_time = (i + 1) * chunk_len\n",
    "        chunk = track[start_time:end_time]\n",
    "        chunk.export(f\"{dest_path}/chunk_{i}.mp3\", format=\"mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract_audio_from_video(\"./files/test2.mp4\", \"./files/test2.mp3\")\n",
    "cut_audio_in_chunks(\"./files/test2.mp3\", 10, \"./files/chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import glob\n",
    "\n",
    "\n",
    "def transcribe_chunks(chunk_folder, destination):\n",
    "    files = glob.glob(f\"{chunk_folder}/*.mp3\")\n",
    "    final_transcript = \"\"\n",
    "    for file in files:\n",
    "        with open(file, \"rb\") as audio_file, open(destination, \"a\") as dest_file:\n",
    "            transcript = openai.Audio.transcribe(\n",
    "                \"whisper-1\",\n",
    "                audio_file,\n",
    "            )\n",
    "            dest_file.write(transcript[\"text\"])\n",
    "\n",
    "\n",
    "transcribe_chunks(\"./files/chunks\", \"./files/transcript.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.tools import StructuredTool\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "\n",
    "def plus(a, b):\n",
    "    return a + b\n",
    "\n",
    "\n",
    "agent = initialize_agent(\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    tools=[\n",
    "        StructuredTool.from_function(\n",
    "            func=plus,\n",
    "            name=\"Sum Calculator\",\n",
    "            description=\"Use this to perform sums of two numbers. This tool take two arguments, both should be numbers\",\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "prompt = \"Cost of $355.23 + $234.23 + $123.23 + $232.23\"\n",
    "\n",
    "agent.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.tools import StructuredTool, Tool\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "\n",
    "def plus(inputs):\n",
    "    a, b = inputs.split(\",\")\n",
    "    return float(a) + float(b)\n",
    "\n",
    "\n",
    "agent = initialize_agent(\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    handle_parsing_errors=True,\n",
    "    tools=[\n",
    "        Tool.from_function(\n",
    "            func=plus,\n",
    "            name=\"Sum Calculator\",\n",
    "            description=\"Use this to perform sums of two numbers. Use this tool by sending a pair of number separated a common.\\nExample:1,2\",\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "prompt = \"Cost of $355.23 + $234.23 + $123.23 + $232.23\"\n",
    "\n",
    "agent.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Type\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.tools import BaseTool\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "\n",
    "class CalculatorToolArgsSchema(BaseModel):\n",
    "    a: float = Field(description=\"The first number\")\n",
    "    b: float = Field(description=\"The second number\")\n",
    "\n",
    "\n",
    "class CalculatorTool(BaseTool):\n",
    "    name = \"SumsCalculator\"\n",
    "    description = \"\"\"\n",
    "    Use this to perform sums of two numbers.\n",
    "    The first and second arguments should be numbers.\n",
    "    Only receives two arguments.\n",
    "    \"\"\"\n",
    "    args_schema: Type[CalculatorToolArgsSchema] = CalculatorToolArgsSchema\n",
    "\n",
    "    def _run(self, a, b):\n",
    "        return a + b\n",
    "\n",
    "\n",
    "agent = initialize_agent(\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    agent=AgentType.OPENAI_FUNCTIONS,\n",
    "    handle_parsing_errors=True,\n",
    "    tools=[CalculatorTool()],\n",
    ")\n",
    "prompt = \"Cost of $355.23 + $234.23 + $123.23 + $232.23\"\n",
    "\n",
    "agent.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Type\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.tools import BaseTool\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "from langchain.utilities import DuckDuckGoSearchAPIWrapper\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "\n",
    "class StockMarketSymbolSearchToolArgsSchema(BaseModel):\n",
    "    query: str = Field(description=\"The query you will serach for\")\n",
    "\n",
    "\n",
    "class StockMarketSymbolSearchTool(BaseTool):\n",
    "    name = \"StockMarketSymbolSearchTool\"\n",
    "    description = \"\"\"Use this tool to find the stock market symbol for a company.\n",
    "    It takes a query as an argument.\n",
    "    Example query: Stock Market Symbol for Apple Company\n",
    "    \"\"\"\n",
    "    args_schema: Type[StockMarketSymbolSearchToolArgsSchema] = (\n",
    "        StockMarketSymbolSearchToolArgsSchema\n",
    "    )\n",
    "\n",
    "    def _run(self, query):\n",
    "        ddg = DuckDuckGoSearchAPIWrapper()\n",
    "        return ddg.run(query)\n",
    "\n",
    "\n",
    "agent = initialize_agent(\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    agent=AgentType.OPENAI_FUNCTIONS,\n",
    "    handle_parsing_errors=True,\n",
    "    tools=[StockMarketSymbolSearchTool()],\n",
    ")\n",
    "prompt = \"Give me information on Cloudflare's stock and help me analyze if it's a potential good investment. Also tell me what symbol does the stock have. in korean\"\n",
    "\n",
    "agent.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from typing import Any, Type\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.tools import BaseTool\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "from langchain.utilities import DuckDuckGoSearchAPIWrapper\n",
    "\n",
    "alpha_vantage_api_key = os.environ.get(\"ALPHA_VANTAGE_API_KEY\")\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "\n",
    "class StockMarketSymbolSearchToolArgsSchema(BaseModel):\n",
    "    query: str = Field(description=\"The query you will serach for\")\n",
    "\n",
    "\n",
    "class StockMarketSymbolSearchTool(BaseTool):\n",
    "    name = \"StockMarketSymbolSearchTool\"\n",
    "    description = \"\"\"Use this tool to find the stock market symbol for a company.\n",
    "    It takes a query as an argument.\n",
    "    Example query: Stock Market Symbol for Apple Company\n",
    "    \"\"\"\n",
    "    args_schema: Type[StockMarketSymbolSearchToolArgsSchema] = (\n",
    "        StockMarketSymbolSearchToolArgsSchema\n",
    "    )\n",
    "\n",
    "    def _run(self, query):\n",
    "        ddg = DuckDuckGoSearchAPIWrapper()\n",
    "        return ddg.run(query)\n",
    "\n",
    "\n",
    "class CompanyOverviewToolArgsSchema(BaseModel):\n",
    "    symbol: str = Field(description=\"Stock symbol of the company.Example: APPL, TSLA\")\n",
    "\n",
    "\n",
    "class CompanyOverviewTool(BaseTool):\n",
    "    name = \"CompanyOverviewTool\"\n",
    "    description = \"\"\"\n",
    "    Use this to get an iverview of the financials of the company.\n",
    "    You should enter a stock symbol\n",
    "    \"\"\"\n",
    "    args_schema: Type[CompanyOverviewToolArgsSchema] = CompanyOverviewToolArgsSchema\n",
    "\n",
    "    def _run(self, symbol):\n",
    "        r = requests.get(\n",
    "            f\"https://www.alphavantage.co/query?function=OVERVIEW&symbol={symbol}&apikey={alpha_vantage_api_key}\"\n",
    "        )\n",
    "        return r.json()\n",
    "\n",
    "\n",
    "class CompanyIncomeStatementTool(BaseTool):\n",
    "    name = \"CompanyIncomeStatementTool\"\n",
    "    description = \"\"\"\n",
    "    Use this to get the income statement of a company.\n",
    "    You should enter a stock symbol\n",
    "    \"\"\"\n",
    "    args_schema: Type[CompanyOverviewToolArgsSchema] = CompanyOverviewToolArgsSchema\n",
    "\n",
    "    def _run(self, symbol):\n",
    "        r = requests.get(\n",
    "            f\"https://www.alphavantage.co/query?function=INCOME_STATEMENT&symbol={symbol}&apikey={alpha_vantage_api_key}\"\n",
    "        )\n",
    "        response = r.json()\n",
    "        return list(response[\"Weekly Time Series\"].items())[:200]\n",
    "\n",
    "\n",
    "class CompanyStockPerformanceTool(BaseTool):\n",
    "    name = \"CompanyStockPerformanceTool\"\n",
    "    description = \"\"\"\n",
    "    Use this to get the weekly performance of a company.\n",
    "    You should enter a stock symbol\n",
    "    \"\"\"\n",
    "    args_schema: Type[CompanyOverviewToolArgsSchema] = CompanyOverviewToolArgsSchema\n",
    "\n",
    "    def _run(self, symbol):\n",
    "        r = requests.get(\n",
    "            f\"https://www.alphavantage.co/query?function=TIME_SERIES_WEEKLY&symbol={symbol}&apikey={alpha_vantage_api_key}\"\n",
    "        )\n",
    "        return r.json()\n",
    "\n",
    "\n",
    "agent = initialize_agent(\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    agent=AgentType.OPENAI_FUNCTIONS,\n",
    "    handle_parsing_errors=True,\n",
    "    tools=[\n",
    "        StockMarketSymbolSearchTool(),\n",
    "        CompanyOverviewTool(),\n",
    "        CompanyIncomeStatementTool(),\n",
    "        CompanyStockPerformanceTool(),\n",
    "    ],\n",
    ")\n",
    "prompt = \"Give me financial information on Cloudflare's stock, considering its financials, income statements and stock performance help me analyze if it's a potential good investment.\"\n",
    "\n",
    "agent.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `sql_db_list_tables` with ``\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3mdirectors, movies\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `sql_db_schema` with `movies`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m\n",
      "CREATE TABLE movies (\n",
      "\tid INTEGER, \n",
      "\toriginal_title VARCHAR, \n",
      "\tbudget INTEGER, \n",
      "\tpopularity INTEGER, \n",
      "\trelease_date TEXT, \n",
      "\trevenue INTEGER, \n",
      "\ttitle TEXT, \n",
      "\tvote_average REAL, \n",
      "\tvote_count INTEGER, \n",
      "\toverview TEXT, \n",
      "\ttagline TEXT, \n",
      "\tuid INTEGER, \n",
      "\tdirector_id INTEGER DEFAULT 0 NOT NULL, \n",
      "\tPRIMARY KEY (id)\n",
      ")\n",
      "\n",
      "/*\n",
      "3 rows from movies table:\n",
      "id\toriginal_title\tbudget\tpopularity\trelease_date\trevenue\ttitle\tvote_average\tvote_count\toverview\ttagline\tuid\tdirector_id\n",
      "43597\tAvatar\t237000000\t150\t2009-12-10\t2787965087\tAvatar\t7.2\t11800\tIn the 22nd century, a paraplegic Marine is dispatched to the moon Pandora on a unique mission, but \tEnter the World of Pandora.\t19995\t4762\n",
      "43598\tPirates of the Caribbean: At World's End\t300000000\t139\t2007-05-19\t961000000\tPirates of the Caribbean: At World's End\t6.9\t4500\tCaptain Barbossa, long believed to be dead, has come back to life and is headed to the edge of the E\tAt the end of the world, the adventure begins.\t285\t4763\n",
      "43599\tSpectre\t245000000\t107\t2015-10-26\t880674609\tSpectre\t6.3\t4466\tA cryptic message from Bond’s past sends him on a trail to uncover a sinister organization. While M \tA Plan No One Escapes\t206647\t4764\n",
      "*/\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `sql_db_query` with `SELECT original_title, director_id, revenue, budget FROM movies ORDER BY vote_count DESC, budget ASC LIMIT 10`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[('Inception', 4765, 825532764, 160000000), ('The Dark Knight', 4765, 1004558444, 185000000), ('Avatar', 4762, 2787965087, 237000000), ('The Avengers', 4769, 1519557910, 220000000), ('Deadpool', 5151, 783112979, 58000000), ('Interstellar', 4765, 675120017, 165000000), ('Django Unchained', 4927, 425368238, 100000000), ('Guardians of the Galaxy', 4827, 773328629, 170000000), ('The Hunger Games', 4973, 691210692, 75000000), ('Mad Max: Fury Road', 4845, 378858340, 150000000)]\u001b[0m\u001b[32;1m\u001b[1;3mThe movies that have the highest votes but the lowest budgets are:\n",
      "\n",
      "1. \"Inception\" directed by director_id 4765, with a revenue of $825,532,764 and a budget of $160,000,000.\n",
      "2. \"The Dark Knight\" directed by director_id 4765, with a revenue of $1,004,558,444 and a budget of $185,000,000.\n",
      "3. \"Avatar\" directed by director_id 4762, with a revenue of $2,787,965,087 and a budget of $237,000,000.\n",
      "4. \"The Avengers\" directed by director_id 4769, with a revenue of $1,519,557,910 and a budget of $220,000,000.\n",
      "5. \"Deadpool\" directed by director_id 5151, with a revenue of $783,112,979 and a budget of $58,000,000.\n",
      "6. \"Interstellar\" directed by director_id 4765, with a revenue of $675,120,017 and a budget of $165,000,000.\n",
      "7. \"Django Unchained\" directed by director_id 4927, with a revenue of $425,368,238 and a budget of $100,000,000.\n",
      "8. \"Guardians of the Galaxy\" directed by director_id 4827, with a revenue of $773,328,629 and a budget of $170,000,000.\n",
      "9. \"The Hunger Games\" directed by director_id 4973, with a revenue of $691,210,692 and a budget of $75,000,000.\n",
      "10. \"Mad Max: Fury Road\" directed by director_id 4845, with a revenue of $378,858,340 and a budget of $150,000,000.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Give me the movies that have the highest votes but the lowest budgets and give me the name of their directors also include their gross revenue.',\n",
       " 'output': 'The movies that have the highest votes but the lowest budgets are:\\n\\n1. \"Inception\" directed by director_id 4765, with a revenue of $825,532,764 and a budget of $160,000,000.\\n2. \"The Dark Knight\" directed by director_id 4765, with a revenue of $1,004,558,444 and a budget of $185,000,000.\\n3. \"Avatar\" directed by director_id 4762, with a revenue of $2,787,965,087 and a budget of $237,000,000.\\n4. \"The Avengers\" directed by director_id 4769, with a revenue of $1,519,557,910 and a budget of $220,000,000.\\n5. \"Deadpool\" directed by director_id 5151, with a revenue of $783,112,979 and a budget of $58,000,000.\\n6. \"Interstellar\" directed by director_id 4765, with a revenue of $675,120,017 and a budget of $165,000,000.\\n7. \"Django Unchained\" directed by director_id 4927, with a revenue of $425,368,238 and a budget of $100,000,000.\\n8. \"Guardians of the Galaxy\" directed by director_id 4827, with a revenue of $773,328,629 and a budget of $170,000,000.\\n9. \"The Hunger Games\" directed by director_id 4973, with a revenue of $691,210,692 and a budget of $75,000,000.\\n10. \"Mad Max: Fury Road\" directed by director_id 4845, with a revenue of $378,858,340 and a budget of $150,000,000.'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import create_sql_agent, AgentType\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents.agent_toolkits import SQLDatabaseToolkit\n",
    "from langchain.sql_database import SQLDatabase\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "db = SQLDatabase.from_uri(\"sqlite:///movies.sqlite\")\n",
    "\n",
    "toolkit = SQLDatabaseToolkit(\n",
    "    db=db,\n",
    "    llm=llm,\n",
    ")\n",
    "\n",
    "agent = create_sql_agent(\n",
    "    llm=llm,\n",
    "    toolkit=toolkit,\n",
    "    agent_type=AgentType.OPENAI_FUNCTIONS,\n",
    "    verbose=True,\n",
    ")\n",
    "agent.invoke(\n",
    "    \"Give me the movies that have the highest votes but the lowest budgets and give me the name of their directors also include their gross revenue.\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
